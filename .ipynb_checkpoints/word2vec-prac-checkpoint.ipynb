{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.chdir('../py')\n",
    "import gc\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from copy import deepcopy\n",
    "from glob import glob\n",
    "\n",
    "import feather\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import japanize_matplotlib\n",
    "import seaborn as sns\n",
    "plt.rcParams[\"patch.force_edgecolor\"] = False\n",
    "plt.rcParams['font.family'] = 'YuGothic'\n",
    "sns.set(style=\"whitegrid\", font='YuGothic', palette=\"muted\",\n",
    "        color_codes=True, rc={'grid.linestyle': '--'})\n",
    "red = sns.xkcd_rgb[\"light red\"]\n",
    "green = sns.xkcd_rgb[\"medium green\"]\n",
    "blue = sns.xkcd_rgb[\"denim blue\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#前処理\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuki.matsumoto/anaconda3/lib/python3.7/site-packages/lightgbm/__init__.py:46: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.3) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "#各種機械学習モデル\n",
    "#LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#Linear Support Vector Machine\n",
    "from sklearn.svm import LinearSVC\n",
    "#Support Vector Machine\n",
    "from sklearn.svm import SVC\n",
    "#決定木\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "#ランダムフォレスト\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#k近傍方（k-neighbor）\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#学習方法（GridSearchCV）\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "#scikiti-learn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import KFold,GroupKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "#スコアリング（f値）\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "#lightgbm\n",
    "import lightgbm as lgb\n",
    "import pandas_profiling as pdp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TfidfVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = np.array([\n",
    "        '牛乳        を 買う',\n",
    "        'パン         を 買う',\n",
    "        'パン         を 食べる',\n",
    "        'お菓子       を 食べる',\n",
    "        '本           を 買う',\n",
    "        'パン と お菓子 を 食べる',\n",
    "        'お菓子        を 買う',\n",
    "        'パン と パン   を 食べる'\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.31957739 0.         0.         0.80024653\n",
      "  0.50742072 0.        ]\n",
      " [0.         0.         0.4068224  0.64594719 0.         0.\n",
      "  0.64594719 0.        ]\n",
      " [0.         0.         0.4068224  0.64594719 0.         0.\n",
      "  0.         0.64594719]\n",
      " [0.69443273 0.         0.38346742 0.         0.         0.\n",
      "  0.         0.60886445]\n",
      " [0.         0.         0.31957739 0.         0.80024653 0.\n",
      "  0.50742072 0.        ]\n",
      " [0.48880235 0.56645286 0.26991783 0.42857192 0.         0.\n",
      "  0.         0.42857192]\n",
      " [0.69443273 0.         0.38346742 0.         0.         0.\n",
      "  0.60886445 0.        ]\n",
      " [0.         0.4945171  0.23564005 0.74829225 0.         0.\n",
      "  0.         0.37414612]]\n"
     ]
    }
   ],
   "source": [
    "# TfidVextrizer\n",
    "#ベクトル化\n",
    "vectorizer = TfidfVectorizer(use_idf=True, token_pattern=u'(?u)\\\\b\\\\w+\\\\b')\n",
    "vecs = vectorizer.fit_transform(docs)\n",
    "print(vecs.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 牛乳        を 買う\n",
      "0 パン         を 買う\n",
      "1 パン         を 食べる\n",
      "1 お菓子       を 食べる\n",
      "0 本           を 買う\n",
      "1 パン と お菓子 を 食べる\n",
      "0 お菓子        を 買う\n",
      "1 パン と パン   を 食べる\n"
     ]
    }
   ],
   "source": [
    "# クラスタリング\n",
    "clusters = KMeans(n_clusters=2, random_state=0).fit_predict(vecs)\n",
    "for doc, cls in zip(docs, clusters):\n",
    "    print(cls, doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文章の前処理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "テキストのベクトル化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "sample = np.array(['Apple computer of the apple mark', 'linux computer', 'windows computer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 7\n",
      "Vocabulary content: {'apple': 0, 'computer': 1, 'of': 4, 'the': 5, 'mark': 3, 'linux': 2, 'windows': 6}\n"
     ]
    }
   ],
   "source": [
    "#CountVectorizer\n",
    "vec_count = CountVectorizer()\n",
    "#ベクトル化\n",
    "vec_count.fit(sample)\n",
    "trans_vec = vec_count.transform(sample)\n",
    "print('Vocabulary size: {}'.format(len(vec_count.vocabulary_)))\n",
    "print('Vocabulary content: {}'.format(vec_count.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>apple</th>\n",
       "      <th>computer</th>\n",
       "      <th>linux</th>\n",
       "      <th>mark</th>\n",
       "      <th>of</th>\n",
       "      <th>the</th>\n",
       "      <th>windows</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   apple  computer  linux  mark  of  the  windows\n",
       "0      2         1      0     1   1    1        0\n",
       "1      0         1      1     0   0    0        0\n",
       "2      0         1      0     0   0    0        1"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(trans_vec.toarray(), columns=vec_count.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これだと出現頻度が高い単語は強くなる\n",
    "次はTF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 7\n",
      "Vocabulary content: {'apple': 0, 'computer': 1, 'of': 4, 'the': 5, 'mark': 3, 'linux': 2, 'windows': 6}\n"
     ]
    }
   ],
   "source": [
    "#TfidfVectorizer\n",
    "vec_tfidf = TfidfVectorizer()\n",
    "#ベクトル化\n",
    "trans_vec_tfidf = vec_tfidf.fit_transform(sample)\n",
    "\n",
    "print('Vocabulary size: {}'.format(len(vec_tfidf.vocabulary_)))\n",
    "print('Vocabulary content: {}'.format(vec_tfidf.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>apple</th>\n",
       "      <th>computer</th>\n",
       "      <th>linux</th>\n",
       "      <th>mark</th>\n",
       "      <th>of</th>\n",
       "      <th>the</th>\n",
       "      <th>windows</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.73777</td>\n",
       "      <td>0.217869</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.368885</td>\n",
       "      <td>0.368885</td>\n",
       "      <td>0.368885</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.508542</td>\n",
       "      <td>0.861037</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.508542</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.861037</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     apple  computer     linux      mark        of       the   windows\n",
       "0  0.73777  0.217869  0.000000  0.368885  0.368885  0.368885  0.000000\n",
       "1  0.00000  0.508542  0.861037  0.000000  0.000000  0.000000  0.000000\n",
       "2  0.00000  0.508542  0.000000  0.000000  0.000000  0.000000  0.861037"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(trans_vec_tfidf.toarray(), columns=vec_tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vecしてみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab\n",
    "\n",
    "class Wakati(object):\n",
    "\n",
    "    \"\"\"\n",
    "    ==========================================================\n",
    "    ファイルに存在する文章を指定の辞書を用いてMeCabによって形態素に分ける\n",
    "    ==========================================================\n",
    "    【関数説明】\n",
    "    __init__ : コンストラクタ\n",
    "    wakati : 文章を分かち書きする\n",
    "    output : 結果をファイルに出力する\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, file_dir, dic_dir=None, user_dir=None, hinshis=[\"動詞\", \"形容詞\", \"形容動詞\", \"助動詞\"]):\n",
    "        \"\"\"\n",
    "        ==========================================================\n",
    "        コンストラクタ\n",
    "        ==========================================================\n",
    "        【変数説明】\n",
    "        file_dir : 入力となる文章のディレクトリ\n",
    "        dic_dir : システム辞書のディレクトリ([ex] /usr/local/lib/mecab/dic/mecab-ipadic-neologd)\n",
    "        user_dir : ユーザー辞書のディレクトリ([ex] /Users/PCのユーザ名/...)\n",
    "        hinshis : 活用する語\n",
    "        tagger : MeCab用のtagger(詳細はMeCab自体のドキュメント参照)\n",
    "        f : 入力ファイル\n",
    "        splited_text : 各行を分かち書きしたもの(splited_line)をリストで格納する(Noneで初期化)\n",
    "        out_dir : 出力ファイルのディレクトリ(Noneで初期化)\n",
    "        \"\"\"\n",
    "        if dic_dir is not None and user_dir is not None:\n",
    "            self.tagger = MeCab.Tagger(\"mecabrc -d {} -u {}\".format(dic_dir, user_dir))\n",
    "        elif dic_dir is not None:\n",
    "            self.tagger = MeCab.Tagger(\"mecabrc -d {}\".format(dic_dir))\n",
    "        else:\n",
    "            self.tagger = MeCab.Tagger(\"mecabrc\")\n",
    "        self.f = open(file_dir, 'r')\n",
    "        self.hinshis = hinshis\n",
    "        self.splited_text = None\n",
    "        self.out_dir = None\n",
    "\n",
    "    def wakati(self):\n",
    "        \"\"\"\n",
    "        ==========================================================\n",
    "        文章全体を分かち書きし、self.splited_textに格納する\n",
    "        その際、活用された語については原形に直す\n",
    "        ==========================================================\n",
    "        【変数説明】\n",
    "        line : 入力文章の一行(更新されていく)\n",
    "        splited_line : 各行の文章を分かち書きしたもののリスト\n",
    "        node : 各単語のノード\n",
    "        word : 単語\n",
    "        feature : 単語の情報\n",
    "        hinshi : 品詞\n",
    "        kata : 活用形\n",
    "        genkei : 原形\n",
    "        \"\"\"\n",
    "        line = self.f.readline()\n",
    "        splited_text = []\n",
    "        while line:\n",
    "            node = self.tagger.parseToNode(line).next\n",
    "            splited_line = []\n",
    "            while node.surface:\n",
    "                word = node.surface\n",
    "                feature = node.feature.split(',')\n",
    "                hinshi = feature[0]\n",
    "                kata = feature[5]\n",
    "                genkei = feature[6]\n",
    "                if hinshi in self.hinshis:\n",
    "                    if kata != \"基本形\":\n",
    "                        word = genkei\n",
    "                splited_line.append(word)\n",
    "                node = node.next\n",
    "            splited_text.append(splited_line)\n",
    "            line = self.f.readline()\n",
    "        self.splited_text = splited_text\n",
    "        self.f.close()\n",
    "\n",
    "    def output(self, out_dir):\n",
    "        \"\"\"\n",
    "        ==========================================================\n",
    "        self.splited_textをファイルに出力する\n",
    "        ==========================================================\n",
    "        【変数説明】\n",
    "        out_dir : 出力ファイルのディレクトリ\n",
    "        fout : 出力ファイル\n",
    "        \"\"\"\n",
    "        assert self.splited_text is not None\n",
    "        if self.out_dir is None:\n",
    "            self.out_dir = out_dir\n",
    "        self.fout = open(self.out_dir, 'w')\n",
    "        for line in self.splited_text:\n",
    "            self.fout.write(\" \".join(line) + \" \")\n",
    "        self.fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "import logging\n",
    "\n",
    "class Vectorizer(Wakati):\n",
    "\n",
    "    \"\"\"\n",
    "    ==========================================================\n",
    "    Wakatiをベースとして、分かち書きしたものを学習して分散表現する\n",
    "    ==========================================================\n",
    "    【関数説明】\n",
    "    __init__ : コンストラクタ\n",
    "    vectorize : 分散表現を作る\n",
    "    _train : gensimを使ってword2vecする\n",
    "    save_model : 作ったモデルを保存する\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, file_dir, dic_dir=None, user_dir=None, out_dir=\"out.txt\", hinshis=[\"動詞\", \"形容詞\", \"形容動詞\", \"助動詞\"]):\n",
    "        \"\"\"\n",
    "        ==========================================================\n",
    "        コンストラクタ\n",
    "        Wakatiを使って文章を分かち書きしておく\n",
    "        ==========================================================\n",
    "        【変数説明】\n",
    "        file_dir : 入力となる文章のディレクトリ\n",
    "        dic_dir : システム辞書のディレクトリ(/usr/local/lib/mecab/dic/mecab-ipadic-neologd)\n",
    "        user_dir : ユーザー辞書のディレクトリ(/Users/ユーザ名/Desktop/word2vec/user.dic)\n",
    "        out_dir : 分かち書きされた文章のファイルのディレクトリ\n",
    "        hinshis : 活用する語\n",
    "        model : モデル(Noneで初期化)\n",
    "        \"\"\"\n",
    "        Wakati.__init__(self, file_dir, dic_dir, user_dir, hinshis)\n",
    "        self.out_dir = out_dir\n",
    "        self.model = None\n",
    "        self.wakati()\n",
    "        self.output(self.out_dir)\n",
    "\n",
    "    def vectorize(self, other_file_dir=None, sg=1, size=300, min_count=10, window=5, hs=0, negative=15, iter=15):\n",
    "        \"\"\"\n",
    "        ==========================================================\n",
    "        単語の分散表現を作成\n",
    "        ==========================================================\n",
    "        【変数説明】\n",
    "        out_dir : 分かち書きされた文章のファイル\n",
    "        other_file_dir : out_dirを使わない場合のファイル名(Noneで初期化)\n",
    "        sentences : 分かち書きされ、空白区切の文章全文\n",
    "        \"\"\"\n",
    "        logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "        if other_file_dir is None:\n",
    "            sentences = word2vec.Text8Corpus(self.out_dir)\n",
    "        else:\n",
    "            sentences = word2vec.Text8Corpus(other_file_dir)\n",
    "        self._train(sentences, sg, size, min_count, window, hs, negative, iter)\n",
    "\n",
    "    def _train(self, sentences, sg=1, size=300, min_count=10, window=5, hs=0, negative=15, iter=15):\n",
    "        \"\"\"\n",
    "        ==========================================================\n",
    "        gensimによる学習\n",
    "        ==========================================================\n",
    "        【変数説明】\n",
    "        sentences : 分かち書きされ、空白区切の文章全文\n",
    "        word2vecの引数 : 詳細はgensimのドキュメント参照\n",
    "        \"\"\"\n",
    "        self.model = word2vec.Word2Vec(sentences, sg=sg, size=size, min_count=min_count, window=window, hs=hs, negative=negative, iter=iter)\n",
    "\n",
    "    def save_model(self, model_dir):\n",
    "        \"\"\"\n",
    "        ==========================================================\n",
    "        モデルを保存する\n",
    "        ==========================================================\n",
    "        【変数説明】\n",
    "        model_dir : モデルを保存するディレクトリ\n",
    "        \"\"\"\n",
    "        assert self.model is not None\n",
    "        self.model.save(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Wakati import Wakati\n",
    "\n",
    "w = Wakati(\"kokoro.txt\", システム辞書のディレクトリ, ユーザ辞書のディレクトリ)\n",
    "w.wakati()\n",
    "w.output(\"out.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Vectorizer import Vectorizer\n",
    "\n",
    "\"\"\"\n",
    "==========================================================\n",
    "自作のWord2Vecを使って、夏目漱石の『こころ』を解析する。\n",
    "(『こころ』は青空文庫(http://www.aozora.gr.jp/cards/000148/card773.html)から\n",
    "ダウンロードして同じディレクトリに入れておく)\n",
    "==========================================================\n",
    "【関数説明】\n",
    "kokoro : 『こころ』のモデルを作成\n",
    "\"\"\"\n",
    "\n",
    "def kokoro():\n",
    "    \"\"\"\n",
    "    ==========================================================\n",
    "    『こころ』から作成されたモデルを返す\n",
    "    ==========================================================\n",
    "    【変数説明】\n",
    "    dic_dir : システム辞書のディレクトリ(このPCでは\"/usr/local/lib/mecab/dic/mecab-ipadic-neologd\")\n",
    "    user_dir : ユーザー辞書のディレクトリ(このPCでは\"/Users/ユーザ名/Desktop/word2vec/user.dic\")\n",
    "    \"\"\"\n",
    "\n",
    "    #Vectorizerインスタンスを作成\n",
    "    #__init__で文章が分かち書きされたファイル(out.txt)が同じディレクトリ内に作られる。\n",
    "    #dic_dir、user_dirを各自のPCに合わせて設定し、Vectorizerの引数にもたせても良い。\n",
    "    v = Vectorizer(file_dir=\"kokoro.txt\")\n",
    "\n",
    "    #単語の分散表現のモデル作成\n",
    "    v.vectorize()\n",
    "\n",
    "    #できたモデルを返す\n",
    "    return v.model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    #『こころ』から作成されたモデル\n",
    "    model = kokoro()\n",
    "\n",
    "    #「人間」という単語に最も近い１０単語を表示する\n",
    "    #resultは(単語, コサイン距離)からなるリスト\n",
    "    result = model.most_similar(positive=\"人間\")\n",
    "    for pair in result:\n",
    "        word = pair[0]\n",
    "        distance = pair[1]\n",
    "        print(word, distance)\n",
    "\n",
    "    #完成したモデルを保存したければ以下のコードのコメントアウトを外す\n",
    "    #v.save_model(\"kokoro.model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "「学問のススメ」を解析してみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mecabで分割\n",
    "from pathlib import Path\n",
    "import MeCab\n",
    "\n",
    "data_dir_path = Path('.').joinpath('input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "元の文：  「天は人の上に人を造らず人の下に人を造らず」と言えり\n",
      "\n",
      "MeCabにより分割後\n",
      "「\t「\t「\t記号-括弧開\t\t\n",
      "天\tテン\t天\t名詞-一般\t\t\n",
      "は\tハ\tは\t助詞-係助詞\t\t\n",
      "人\tヒト\t人\t名詞-一般\t\t\n",
      "の\tノ\tの\t助詞-連体化\t\t\n",
      "上\tウエ\t上\t名詞-非自立-副詞可能\t\t\n",
      "に\tニ\tに\t助詞-格助詞-一般\t\t\n",
      "人\tヒト\t人\t名詞-一般\t\t\n",
      "を\tヲ\tを\t助詞-格助詞-一般\t\t\n",
      "造ら\tツクラ\t造る\t動詞-自立\t五段・ラ行\t未然形\n",
      "ず\tズ\tぬ\t助動詞\t特殊・ヌ\t連用ニ接続\n",
      "人\tヒト\t人\t名詞-一般\t\t\n",
      "の\tノ\tの\t助詞-連体化\t\t\n",
      "下\tシタ\t下\t名詞-一般\t\t\n",
      "に\tニ\tに\t助詞-格助詞-一般\t\t\n",
      "人\tヒト\t人\t名詞-一般\t\t\n",
      "を\tヲ\tを\t助詞-格助詞-一般\t\t\n",
      "造ら\tツクラ\t造る\t動詞-自立\t五段・ラ行\t未然形\n",
      "ず\tズ\tぬ\t助動詞\t特殊・ヌ\t連用ニ接続\n",
      "」\t」\t」\t記号-括弧閉\t\t\n",
      "と\tト\tと\t助詞-格助詞-引用\t\t\n",
      "言え\tイエ\t言える\t動詞-自立\t一段\t連用形\n",
      "り\tリ\tり\t助動詞\t文語・リ\t基本形\n",
      "EOS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tagger = MeCab.Tagger(\"-Ochasen\")\n",
    "tagger.parse('')\n",
    "\n",
    "#  「学問のすすめ」のデータを読み込む\n",
    "with open(data_dir_path.joinpath('gakumonno_susume.txt'), 'r', encoding='shift-jis') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "sentences = []\n",
    "for sentence in lines:\n",
    "    texts = sentence.split('。')\n",
    "    sentences.extend(texts)\n",
    "    \n",
    "sentence = sentences[23]\n",
    "words = tagger.parse(sentence)\n",
    "\n",
    "print('元の文： ', sentence)\n",
    "print('')\n",
    "print('MeCabにより分割後')\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文においてはそれぞれの数字の値に意味がある場合もありますが、単語の関連性を分析するうえでは数字は数字として一つにまとめた方が良い"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文章の単語出現頻度を可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
